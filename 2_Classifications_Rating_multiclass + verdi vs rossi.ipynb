{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, make_scorer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "path = r'C:\\Users\\chiar\\Documents\\Università\\Text analytics\\Data'\n",
    "#path = r'D:\\tirocinioLC\\tirocinioLC'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-processed corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 37693 entries, 1 to 54718\n",
      "Data columns (total 8 columns):\n",
      " #   Column   Non-Null Count  Dtype              \n",
      "---  ------   --------------  -----              \n",
      " 0   ID       37693 non-null  int64              \n",
      " 1   Title    37690 non-null  object             \n",
      " 2   Rating   37693 non-null  object             \n",
      " 3   Author   37693 non-null  object             \n",
      " 4   Date     37693 non-null  datetime64[ns, UTC]\n",
      " 5   Chapter  37693 non-null  int64              \n",
      " 6   Text     37693 non-null  object             \n",
      " 7   N_Rev    37693 non-null  int64              \n",
      "dtypes: datetime64[ns, UTC](1), int64(3), object(4)\n",
      "memory usage: 2.6+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Author</th>\n",
       "      <th>Date</th>\n",
       "      <th>Chapter</th>\n",
       "      <th>Text</th>\n",
       "      <th>N_Rev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2909917</td>\n",
       "      <td>Rilassati! Hai tutta la morte davanti!</td>\n",
       "      <td>verde</td>\n",
       "      <td>Tonks98</td>\n",
       "      <td>2014-11-15 00:00:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Zi zieda, prego. Allora, quale ezzere zuo  \"B...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1390250</td>\n",
       "      <td>Episodi della Old Generation 1.</td>\n",
       "      <td>verde</td>\n",
       "      <td>mrsreg</td>\n",
       "      <td>2012-11-17 00:00:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Introduzione.     Personaggi:     Argus Gazza:...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1143283</td>\n",
       "      <td>In Noctem</td>\n",
       "      <td>verde</td>\n",
       "      <td>LilacLilium</td>\n",
       "      <td>2012-04-07 00:00:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Questo mio piccolo lavoretto è ispirato a una ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>917615</td>\n",
       "      <td>Dirty flower.</td>\n",
       "      <td>verde</td>\n",
       "      <td>Rue</td>\n",
       "      <td>2012-07-01 00:00:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>DIRTY FLOWER.     Lo hai sempre saputo, Lily. ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>917635</td>\n",
       "      <td>Hawthorn and Unicorn Air</td>\n",
       "      <td>giallo</td>\n",
       "      <td>Tonna</td>\n",
       "      <td>2012-08-01 00:00:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Before you read:     Bentro...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID                                   Title  Rating       Author  \\\n",
       "1  2909917  Rilassati! Hai tutta la morte davanti!   verde      Tonks98   \n",
       "4  1390250         Episodi della Old Generation 1.   verde       mrsreg   \n",
       "5  1143283                               In Noctem   verde  LilacLilium   \n",
       "7   917615                           Dirty flower.   verde          Rue   \n",
       "8   917635                Hawthorn and Unicorn Air  giallo        Tonna   \n",
       "\n",
       "                       Date  Chapter  \\\n",
       "1 2014-11-15 00:00:00+00:00        1   \n",
       "4 2012-11-17 00:00:00+00:00        1   \n",
       "5 2012-04-07 00:00:00+00:00        1   \n",
       "7 2012-07-01 00:00:00+00:00        1   \n",
       "8 2012-08-01 00:00:00+00:00        1   \n",
       "\n",
       "                                                Text  N_Rev  \n",
       "1  \"Zi zieda, prego. Allora, quale ezzere zuo  \"B...      3  \n",
       "4  Introduzione.     Personaggi:     Argus Gazza:...      0  \n",
       "5  Questo mio piccolo lavoretto è ispirato a una ...      2  \n",
       "7  DIRTY FLOWER.     Lo hai sempre saputo, Lily. ...      3  \n",
       "8                     Before you read:     Bentro...      4  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(path+'\\df_final.json')\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.Rating.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "verde        26104\n",
       "giallo        6927\n",
       "arancione     2623\n",
       "rosso         2039\n",
       "Name: Rating, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdf = df[(df['Rating']!='giallo') & (df['Rating']!='arancione')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df[['Rating']]\n",
    "del df['Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split on train-test \n",
    "x_train, x_test, y_train, y_test = train_test_split(df, target, test_size=0.30, random_state=42, stratify=target, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 26385\n",
      "Test set size: 11308\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train set size: {len(x_train)}\\nTest set size: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction with nltk and spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download it_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "nlp = spacy.load('it_core_news_sm')\n",
    "\n",
    "doc_counter = 0\n",
    "def reset_counter():\n",
    "  global doc_counter\n",
    "  doc_counter = 0\n",
    "\n",
    "def increase_counter():\n",
    "  global doc_counter\n",
    "  doc_counter += 1\n",
    "  if doc_counter % 100 == 0:\n",
    "    print(doc_counter)\n",
    "\n",
    "def spacy_nlp_tokenizer(text):\n",
    "    increase_counter()\n",
    "\n",
    "    # substituting all space characters with a single space\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "\n",
    "    # we use spacy for main nlp tasks\n",
    "    doc = nlp(text)\n",
    "    # lemmatized tokens, skipping stopwords\n",
    "    lemmas = ['LEMMA_'+token.lemma_ for token in doc if not token.is_stop]\n",
    "    # entity_types\n",
    "    entity_types = ['NER_'+token.ent_type_ for token in doc if token.ent_type_]\n",
    "\n",
    "    # in case an entity linker is available, we can use it do put actual entities as\n",
    "    # features, e.g. Queen Elizabeth, Elizabeth II, Her Majesty -> KB2912\n",
    "    # see https://spacy.io/usage/training#entity-linker\n",
    "    # entities = ['ENT_'+token.ent_kb_id_ for token in doc if token.ent_kb_id_]\n",
    "\n",
    "    # we use a simple nltk function to create ngrams\n",
    "    lemma_bigrams = ['BI_'+p1+'_'+p2 for p1,p2 in nltk.ngrams(lemmas,2)]\n",
    "    lemma_trigrams = ['TRI_'+p1+'_'+p2+'_'+p3 for p1,p2,p3 in nltk.ngrams(lemmas,3)]\n",
    "\n",
    "    all_tokens = list()\n",
    "    all_tokens.extend(lemmas)\n",
    "    all_tokens.extend(lemma_bigrams)\n",
    "    all_tokens.extend(lemma_trigrams)\n",
    "    all_tokens.extend(entity_types)\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#vect = CountVectorizer(analyzer=spacy_nlp_tokenizer, min_df=5)  \n",
    "#reset_counter()\n",
    "#X_train_tok = vect.fit_transform(x_train.Racconto_Text_Only)\n",
    "#reset_counter()\n",
    "#X_test_tok = vect.transform(x_test.Racconto_Text_Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(path+r'\\Rating\\multiclass\\x_train_tok.pkl','wb') as outfile:\n",
    "#    pickle.dump(X_train_tok, outfile)\n",
    "#with open(path+r'\\Rating\\multiclass\\x_test_tok.pkl','wb') as outfile:\n",
    "#    pickle.dump(X_test_tok, outfile)\n",
    "#with open(path+r'\\Rating\\multiclass\\vect.pkl','wb') as outfile:\n",
    "#    pickle.dump(vect, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = open(path+r\"\\Rating\\multiclass\\x_train_tok.pkl\", \"rb\")\n",
    "X_train_tok = pickle.load(train_file)\n",
    "\n",
    "test_file = open(path+r\"\\Rating\\multiclass\\x_test_tok.pkl\", \"rb\")\n",
    "X_test_tok = pickle.load(test_file)\n",
    "\n",
    "vect_file = open(path+r\"\\Rating\\multiclass\\vect.pkl\", \"rb\")\n",
    "vect = pickle.load(vect_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lunghezza del vocabolario: 914227\n"
     ]
    }
   ],
   "source": [
    "print(f\"Lunghezza del vocabolario: {len(vect.vocabulary_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rating Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('sel', SelectKBest(chi2, k='all')),  # feature selection\n",
    "    ('tfidf', TfidfTransformer()),  # weighting\n",
    "    ('learner', LinearSVC())  # learning algorithm\n",
    "])\n",
    "\n",
    "reset_counter()\n",
    "pipeline.fit(X_train_tok, y_train.values.ravel())\n",
    "\n",
    "reset_counter()\n",
    "predictions = pipeline.predict(X_test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   arancione       0.50      0.03      0.06       787\n",
      "      giallo       0.43      0.14      0.21      2078\n",
      "       rosso       0.77      0.64      0.70       612\n",
      "       verde       0.76      0.98      0.86      7831\n",
      "\n",
      "    accuracy                           0.74     11308\n",
      "   macro avg       0.61      0.45      0.46     11308\n",
      "weighted avg       0.68      0.74      0.67     11308\n",
      "\n",
      "Confusion matrix:\n",
      "[[  26  171   81  509]\n",
      " [  12  285   24 1757]\n",
      " [   8   61  389  154]\n",
      " [   6  150   12 7663]]\n"
     ]
    }
   ],
   "source": [
    "print('Classification report:')\n",
    "print(classification_report(y_test, predictions))\n",
    "print('Confusion matrix:')\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = [{'sel__k': [10000, 100000, 250000, 500000, 'all'],\n",
    "                 'learner':[MultinomialNB()],\n",
    "                 'learner__alpha': [1e-3, 1e-2, 0.1, 1.0, 10.0],\n",
    "                 'learner__fit_prior':[True, False]}, \n",
    "                {'sel__k': [10000, 100000, 250000, 500000, 'all'],\n",
    "                 'learner':[LinearSVC()],\n",
    "                 'learner__C': [0.01, 0.1, 1, 10, 100]},\n",
    "                {'sel__k': [10000, 100000, 250000, 500000, 'all'], \n",
    "                 'learner':[LogisticRegression()],\n",
    "                 'learner__solver':[\"liblinear\"],\n",
    "                 'learner__penalty' : ['l1', 'l2'],\n",
    "                 'learner__C': [0.01, 0.1, 1, 10, 100]} \n",
    "               ]\n",
    "\n",
    "opt_pipeline = Pipeline([\n",
    "#    ('vect', CountVectorizer()),  # feature extraction\n",
    "    ('sel', SelectKBest(chi2)),  # feature selection\n",
    "    ('tfidf', TfidfTransformer()),  # weighting\n",
    "    ('learner', LinearSVC())  # learning algorithm\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chiar\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "opt_search = GridSearchCV(opt_pipeline, \n",
    "                                      search_space,\n",
    "                                      cv=3, n_jobs = 4, verbose=True).fit(X_train_tok, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       " 'learner__C': 100,\n",
       " 'learner__penalty': 'l2',\n",
       " 'learner__solver': 'liblinear',\n",
       " 'sel__k': 'all'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([   3.62208446,    4.5288926 ,    5.13820759,    7.32212281,\n",
       "           9.63475633,    3.51315991,    4.14589715,    5.09021211,\n",
       "           7.236329  ,    9.88184897,    3.55349048,    3.87724368,\n",
       "           4.86170475,    7.42594592,    9.38192312,    3.27139584,\n",
       "           3.90982978,    5.52135396,    7.77439483,   11.54193298,\n",
       "           3.54549615,    4.16753523,    5.50020806,    7.78270006,\n",
       "          10.56871255,    3.77418478,    4.24977859,    4.69897715,\n",
       "           7.3731451 ,   10.05078236,    3.4598647 ,    4.17657423,\n",
       "           5.11354359,    6.97605443,   10.35176341,    3.48587894,\n",
       "           3.981668  ,    4.88995043,    7.73618698,   10.58065017,\n",
       "           4.00749516,    4.72816777,    4.26327173,    5.98708582,\n",
       "           8.5526437 ,    2.81130918,    3.34975123,    4.14591249,\n",
       "           6.55081177,    8.50759292,    4.0377655 ,    5.25734639,\n",
       "           6.81623212,   13.07578214,   20.67974758,    5.90473056,\n",
       "           6.11120391,    8.44493715,   14.96185637,   22.89740364,\n",
       "           9.44266669,   10.92760984,   18.90212631,   37.62051717,\n",
       "          55.97813829,   33.532945  ,   41.45431527,  101.43714229,\n",
       "         242.15409088,  340.22108706,   97.05956133,  102.68756183,\n",
       "         264.44834717,  660.62029076, 1077.09351746,    7.06925289,\n",
       "          12.64264123,   19.52392205,   36.76354074,   51.43985256,\n",
       "           6.87336183,   11.4266932 ,   19.30619351,   32.49973273,\n",
       "          49.57563647,    8.52394112,   11.77994657,   19.7015392 ,\n",
       "          34.46214382,   50.2784524 ,    6.14772121,    9.76024763,\n",
       "          17.65214181,   37.63793691,   60.17021465,   14.79512239,\n",
       "          12.06231085,   15.6765426 ,   25.16605759,   39.6148266 ,\n",
       "          10.42795666,   16.75441758,   34.30000854,   75.45473584,\n",
       "          98.57597502,   54.31595739,   30.46251448,   32.21184794,\n",
       "          40.92737198,   56.15717999,   17.11790625,   32.98636969,\n",
       "          66.81874863,  146.68865561,  192.35360066,  153.03045909,\n",
       "          45.71301643,   45.31048822,   53.77646192,   69.86731768,\n",
       "          34.94863399,   56.4301885 ,   98.96679632,  219.75812014,\n",
       "         248.16518394]),\n",
       " 'std_fit_time': array([1.73466137e-02, 9.42034880e-02, 3.60531439e-01, 2.79322226e-01,\n",
       "        6.29969440e-01, 1.82596985e-01, 2.35455055e-01, 3.17830648e-01,\n",
       "        3.01552250e-01, 4.78185934e-01, 2.74700587e-01, 5.96245944e-02,\n",
       "        7.40679769e-02, 1.81036805e-01, 1.58698618e-01, 3.03024952e-02,\n",
       "        2.38826793e-01, 3.86776797e-01, 1.08458037e+00, 6.44081783e-01,\n",
       "        8.83540750e-02, 1.29707169e-01, 4.64477149e-02, 2.03958901e-01,\n",
       "        5.65762926e-01, 2.92397659e-01, 1.77089842e-01, 1.55382087e-01,\n",
       "        2.22218073e-01, 4.36055934e-01, 1.94522554e-01, 4.46807488e-02,\n",
       "        1.81873054e-02, 5.83342542e-01, 5.86028028e-01, 6.31732640e-02,\n",
       "        6.02071694e-02, 1.19390184e-01, 3.88643641e-01, 1.11810566e+00,\n",
       "        1.45441016e-01, 8.18428392e-02, 9.92021420e-02, 2.59670651e-01,\n",
       "        3.48667939e-01, 1.68004584e-01, 3.12690222e-01, 8.79069242e-02,\n",
       "        6.69261213e-01, 5.01976297e-01, 1.64922442e-01, 3.40038234e-01,\n",
       "        4.43754520e-01, 1.49176305e+00, 7.21667799e-01, 3.96235684e-01,\n",
       "        5.63318744e-01, 3.65905347e-01, 1.90487667e+00, 1.27635542e+00,\n",
       "        1.87323153e+00, 3.68624881e-01, 1.40867527e+00, 4.51400110e+00,\n",
       "        4.48795090e+00, 8.61426264e+00, 5.12081372e-01, 1.29381559e+01,\n",
       "        2.66881969e+01, 4.04458941e+01, 1.98993733e+01, 1.10018429e+00,\n",
       "        4.78417054e+01, 9.22554440e+01, 3.81325952e+01, 2.23506426e-01,\n",
       "        4.75705474e-01, 4.11691934e-01, 1.89895678e+00, 8.36140706e+00,\n",
       "        3.17545401e-01, 4.00799448e-01, 8.09848491e-01, 7.52051383e-01,\n",
       "        2.33516219e+00, 1.76996180e+00, 5.25727976e-01, 4.72069315e-01,\n",
       "        4.26313171e-01, 1.70107078e+00, 4.43232349e-02, 5.79020440e-01,\n",
       "        2.24899323e+00, 2.19650606e+00, 2.80995756e+00, 1.63370850e+00,\n",
       "        1.42674079e+00, 7.97243947e-01, 3.29049683e-01, 6.14952201e-01,\n",
       "        4.17189372e-01, 7.35331089e-01, 3.20238748e+00, 6.78964306e+00,\n",
       "        1.15975299e+01, 1.55092924e+01, 6.12623081e-01, 4.97704673e-01,\n",
       "        6.69785702e-01, 1.86243175e+00, 6.79477412e-01, 9.45932312e-01,\n",
       "        2.74039621e+00, 8.34147166e+00, 2.50816755e+01, 3.62557818e+01,\n",
       "        1.28220132e+00, 2.17120003e+00, 1.19264460e+00, 8.89796794e-01,\n",
       "        2.22602395e+00, 1.08282810e+00, 9.84321928e+00, 3.73143367e+00,\n",
       "        3.41156029e+01]),\n",
       " 'mean_score_time': array([0.59922663, 1.23798092, 1.99456763, 2.72136855, 3.69478154,\n",
       "        0.72888931, 1.18188659, 1.88842463, 2.90598567, 3.68180251,\n",
       "        0.67115108, 1.10299627, 2.02461092, 2.70309909, 3.46676644,\n",
       "        0.80135798, 1.05734793, 1.82827719, 3.44829845, 3.89915673,\n",
       "        0.70551896, 1.14198287, 1.89872797, 3.35324057, 4.26589179,\n",
       "        0.7453235 , 1.01390092, 1.92996558, 2.62167851, 3.64513659,\n",
       "        0.82453442, 1.40677055, 1.96154277, 3.03487659, 3.85317039,\n",
       "        0.77145036, 1.16458193, 1.67855922, 2.55855767, 3.87442517,\n",
       "        0.7196095 , 0.91467706, 1.42665482, 2.81722943, 3.02687907,\n",
       "        0.59456968, 1.02710183, 1.33167473, 2.19656515, 3.01769908,\n",
       "        0.58070993, 0.93765903, 1.55978489, 2.79589176, 3.28054841,\n",
       "        0.66981276, 0.87511675, 1.4444743 , 3.02898637, 3.01147819,\n",
       "        0.50877134, 1.04019324, 1.67782966, 3.44249129, 2.87191017,\n",
       "        0.44022282, 0.8213493 , 1.85692906, 3.32086054, 2.99816934,\n",
       "        0.58561468, 0.8363866 , 1.70967555, 3.42827249, 3.24225791,\n",
       "        0.84448028, 1.3865908 , 2.5175972 , 3.8828342 , 4.65251883,\n",
       "        0.73314897, 1.29689662, 2.2543606 , 3.58381136, 4.4908247 ,\n",
       "        0.73294266, 1.04027502, 1.72291247, 2.34926399, 2.86384281,\n",
       "        0.48415605, 0.87572281, 1.71515648, 3.32032434, 3.12744713,\n",
       "        0.67692494, 0.82475138, 1.1872526 , 1.82395546, 2.55613971,\n",
       "        0.52354678, 1.010499  , 1.90786179, 3.35804947, 2.85484306,\n",
       "        0.48146709, 0.78201866, 1.16731668, 1.86910979, 2.72745283,\n",
       "        0.48417187, 1.07406855, 1.94863359, 3.45139027, 2.97516402,\n",
       "        0.48513381, 0.77433276, 1.17777816, 1.69805082, 2.39814631,\n",
       "        0.55025331, 1.06462614, 1.89577254, 3.54350551, 2.48780696]),\n",
       " 'std_score_time': array([0.01028871, 0.04147728, 0.21269792, 0.26197408, 0.30957301,\n",
       "        0.04875204, 0.09430543, 0.12402912, 0.20681662, 0.46431092,\n",
       "        0.03001649, 0.09287398, 0.19152376, 0.44398703, 0.29728599,\n",
       "        0.04383448, 0.0896932 , 0.19567075, 0.33966718, 0.34688492,\n",
       "        0.04454352, 0.11810537, 0.16251521, 0.0334708 , 0.48673993,\n",
       "        0.06607941, 0.03056284, 0.23980648, 0.49113031, 0.34892076,\n",
       "        0.09255929, 0.05724597, 0.239853  , 0.46840448, 0.46659169,\n",
       "        0.05669817, 0.08294405, 0.19236576, 0.05252119, 0.61283866,\n",
       "        0.08407998, 0.06392093, 0.07950263, 0.52584199, 0.4947608 ,\n",
       "        0.06504035, 0.05917745, 0.13712122, 0.33663829, 0.52526861,\n",
       "        0.05991137, 0.10079765, 0.17016592, 0.13555412, 0.74089021,\n",
       "        0.11386265, 0.10036748, 0.17009364, 0.16203156, 0.63033346,\n",
       "        0.01676928, 0.12091581, 0.10369123, 0.17091069, 0.76855762,\n",
       "        0.00913968, 0.05374238, 0.22872143, 0.31485003, 0.76411513,\n",
       "        0.08297123, 0.05805102, 0.08809277, 0.11139301, 0.76973906,\n",
       "        0.03200493, 0.15480179, 0.19955174, 0.37782747, 0.72829092,\n",
       "        0.05645348, 0.06828204, 0.15346962, 0.20611893, 0.93329629,\n",
       "        0.1491425 , 0.09922452, 0.19176352, 0.24921981, 0.17462817,\n",
       "        0.02104982, 0.00728337, 0.08639982, 0.06180693, 0.61468892,\n",
       "        0.06435894, 0.01649954, 0.06643976, 0.11325155, 0.03675682,\n",
       "        0.07094395, 0.07504817, 0.2936463 , 0.0390565 , 0.70945657,\n",
       "        0.04345573, 0.04555694, 0.05853042, 0.11769403, 0.15493951,\n",
       "        0.05832506, 0.03099256, 0.17183435, 0.14097716, 0.7685169 ,\n",
       "        0.0507295 , 0.01722263, 0.12800382, 0.04191827, 0.17936929,\n",
       "        0.04059677, 0.04616798, 0.16439622, 0.27026025, 1.05402836]),\n",
       " 'param_learner': masked_array(data=[MultinomialNB(), MultinomialNB(), MultinomialNB(),\n",
       "                    MultinomialNB(), MultinomialNB(), MultinomialNB(),\n",
       "                    MultinomialNB(), MultinomialNB(), MultinomialNB(),\n",
       "                    MultinomialNB(), MultinomialNB(), MultinomialNB(),\n",
       "                    MultinomialNB(), MultinomialNB(), MultinomialNB(),\n",
       "                    MultinomialNB(), MultinomialNB(), MultinomialNB(),\n",
       "                    MultinomialNB(), MultinomialNB(), MultinomialNB(),\n",
       "                    MultinomialNB(), MultinomialNB(), MultinomialNB(),\n",
       "                    MultinomialNB(), MultinomialNB(), MultinomialNB(),\n",
       "                    MultinomialNB(), MultinomialNB(), MultinomialNB(),\n",
       "                    MultinomialNB(), MultinomialNB(), MultinomialNB(),\n",
       "                    MultinomialNB(), MultinomialNB(), MultinomialNB(),\n",
       "                    MultinomialNB(), MultinomialNB(), MultinomialNB(),\n",
       "                    MultinomialNB(), MultinomialNB(), MultinomialNB(),\n",
       "                    MultinomialNB(), MultinomialNB(), MultinomialNB(),\n",
       "                    MultinomialNB(), MultinomialNB(), MultinomialNB(),\n",
       "                    MultinomialNB(), MultinomialNB(), LinearSVC(),\n",
       "                    LinearSVC(), LinearSVC(), LinearSVC(), LinearSVC(),\n",
       "                    LinearSVC(), LinearSVC(), LinearSVC(), LinearSVC(),\n",
       "                    LinearSVC(), LinearSVC(), LinearSVC(), LinearSVC(),\n",
       "                    LinearSVC(), LinearSVC(), LinearSVC(), LinearSVC(),\n",
       "                    LinearSVC(), LinearSVC(), LinearSVC(), LinearSVC(),\n",
       "                    LinearSVC(), LinearSVC(), LinearSVC(), LinearSVC(),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear'),\n",
       "                    LogisticRegression(C=100, solver='liblinear')],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_learner__alpha': masked_array(data=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,\n",
       "                    0.001, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
       "                    1.0, 1.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0,\n",
       "                    10.0, 10.0, 10.0, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_learner__fit_prior': masked_array(data=[True, True, True, True, True, False, False, False,\n",
       "                    False, False, True, True, True, True, True, False,\n",
       "                    False, False, False, False, True, True, True, True,\n",
       "                    True, False, False, False, False, False, True, True,\n",
       "                    True, True, True, False, False, False, False, False,\n",
       "                    True, True, True, True, True, False, False, False,\n",
       "                    False, False, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_sel__k': masked_array(data=[10000, 100000, 250000, 500000, 'all', 10000, 100000,\n",
       "                    250000, 500000, 'all', 10000, 100000, 250000, 500000,\n",
       "                    'all', 10000, 100000, 250000, 500000, 'all', 10000,\n",
       "                    100000, 250000, 500000, 'all', 10000, 100000, 250000,\n",
       "                    500000, 'all', 10000, 100000, 250000, 500000, 'all',\n",
       "                    10000, 100000, 250000, 500000, 'all', 10000, 100000,\n",
       "                    250000, 500000, 'all', 10000, 100000, 250000, 500000,\n",
       "                    'all', 10000, 100000, 250000, 500000, 'all', 10000,\n",
       "                    100000, 250000, 500000, 'all', 10000, 100000, 250000,\n",
       "                    500000, 'all', 10000, 100000, 250000, 500000, 'all',\n",
       "                    10000, 100000, 250000, 500000, 'all', 10000, 100000,\n",
       "                    250000, 500000, 'all', 10000, 100000, 250000, 500000,\n",
       "                    'all', 10000, 100000, 250000, 500000, 'all', 10000,\n",
       "                    100000, 250000, 500000, 'all', 10000, 100000, 250000,\n",
       "                    500000, 'all', 10000, 100000, 250000, 500000, 'all',\n",
       "                    10000, 100000, 250000, 500000, 'all', 10000, 100000,\n",
       "                    250000, 500000, 'all', 10000, 100000, 250000, 500000,\n",
       "                    'all', 10000, 100000, 250000, 500000, 'all'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_learner__C': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 1, 1, 1, 1, 1, 10, 10,\n",
       "                    10, 10, 10, 100, 100, 100, 100, 100, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    100, 100, 100, 100, 100, 100, 100, 100, 100, 100],\n",
       "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_learner__penalty': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, 'l1', 'l1', 'l1', 'l1', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l1', 'l1', 'l1', 'l1', 'l1',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l1', 'l1', 'l1', 'l1',\n",
       "                    'l1', 'l2', 'l2', 'l2', 'l2', 'l2', 'l1', 'l1', 'l1',\n",
       "                    'l1', 'l1', 'l2', 'l2', 'l2', 'l2', 'l2', 'l1', 'l1',\n",
       "                    'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2', 'l2'],\n",
       "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_learner__solver': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'liblinear'],\n",
       "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'learner': MultinomialNB(),\n",
       "   'learner__alpha': 0.001,\n",
       "   'learner__fit_prior': True,\n",
       "   'sel__k': 10000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 0.001,\n",
       "   'learner__fit_prior': True,\n",
       "   'sel__k': 100000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 0.001,\n",
       "   'learner__fit_prior': True,\n",
       "   'sel__k': 250000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 0.001,\n",
       "   'learner__fit_prior': True,\n",
       "   'sel__k': 500000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 0.001,\n",
       "   'learner__fit_prior': True,\n",
       "   'sel__k': 'all'},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 0.001,\n",
       "   'learner__fit_prior': False,\n",
       "   'sel__k': 10000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 0.001,\n",
       "   'learner__fit_prior': False,\n",
       "   'sel__k': 100000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 0.001,\n",
       "   'learner__fit_prior': False,\n",
       "   'sel__k': 250000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 0.001,\n",
       "   'learner__fit_prior': False,\n",
       "   'sel__k': 500000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 0.001,\n",
       "   'learner__fit_prior': False,\n",
       "   'sel__k': 'all'},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 0.01,\n",
       "   'learner__fit_prior': True,\n",
       "   'sel__k': 10000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 0.01,\n",
       "   'learner__fit_prior': True,\n",
       "   'sel__k': 100000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 0.01,\n",
       "   'learner__fit_prior': True,\n",
       "   'sel__k': 250000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 0.01,\n",
       "   'learner__fit_prior': True,\n",
       "   'sel__k': 500000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 0.01,\n",
       "   'learner__fit_prior': True,\n",
       "   'sel__k': 'all'},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 0.01,\n",
       "   'learner__fit_prior': False,\n",
       "   'sel__k': 10000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 0.01,\n",
       "   'learner__fit_prior': False,\n",
       "   'sel__k': 100000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 0.01,\n",
       "   'learner__fit_prior': False,\n",
       "   'sel__k': 250000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 0.01,\n",
       "   'learner__fit_prior': False,\n",
       "   'sel__k': 500000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 0.01,\n",
       "   'learner__fit_prior': False,\n",
       "   'sel__k': 'all'},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 0.1,\n",
       "   'learner__fit_prior': True,\n",
       "   'sel__k': 10000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 0.1,\n",
       "   'learner__fit_prior': True,\n",
       "   'sel__k': 100000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 0.1,\n",
       "   'learner__fit_prior': True,\n",
       "   'sel__k': 250000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 0.1,\n",
       "   'learner__fit_prior': True,\n",
       "   'sel__k': 500000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 0.1,\n",
       "   'learner__fit_prior': True,\n",
       "   'sel__k': 'all'},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 0.1,\n",
       "   'learner__fit_prior': False,\n",
       "   'sel__k': 10000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 0.1,\n",
       "   'learner__fit_prior': False,\n",
       "   'sel__k': 100000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 0.1,\n",
       "   'learner__fit_prior': False,\n",
       "   'sel__k': 250000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 0.1,\n",
       "   'learner__fit_prior': False,\n",
       "   'sel__k': 500000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 0.1,\n",
       "   'learner__fit_prior': False,\n",
       "   'sel__k': 'all'},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 1.0,\n",
       "   'learner__fit_prior': True,\n",
       "   'sel__k': 10000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 1.0,\n",
       "   'learner__fit_prior': True,\n",
       "   'sel__k': 100000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 1.0,\n",
       "   'learner__fit_prior': True,\n",
       "   'sel__k': 250000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 1.0,\n",
       "   'learner__fit_prior': True,\n",
       "   'sel__k': 500000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 1.0,\n",
       "   'learner__fit_prior': True,\n",
       "   'sel__k': 'all'},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 1.0,\n",
       "   'learner__fit_prior': False,\n",
       "   'sel__k': 10000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 1.0,\n",
       "   'learner__fit_prior': False,\n",
       "   'sel__k': 100000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 1.0,\n",
       "   'learner__fit_prior': False,\n",
       "   'sel__k': 250000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 1.0,\n",
       "   'learner__fit_prior': False,\n",
       "   'sel__k': 500000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 1.0,\n",
       "   'learner__fit_prior': False,\n",
       "   'sel__k': 'all'},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 10.0,\n",
       "   'learner__fit_prior': True,\n",
       "   'sel__k': 10000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 10.0,\n",
       "   'learner__fit_prior': True,\n",
       "   'sel__k': 100000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 10.0,\n",
       "   'learner__fit_prior': True,\n",
       "   'sel__k': 250000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 10.0,\n",
       "   'learner__fit_prior': True,\n",
       "   'sel__k': 500000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 10.0,\n",
       "   'learner__fit_prior': True,\n",
       "   'sel__k': 'all'},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 10.0,\n",
       "   'learner__fit_prior': False,\n",
       "   'sel__k': 10000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 10.0,\n",
       "   'learner__fit_prior': False,\n",
       "   'sel__k': 100000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 10.0,\n",
       "   'learner__fit_prior': False,\n",
       "   'sel__k': 250000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 10.0,\n",
       "   'learner__fit_prior': False,\n",
       "   'sel__k': 500000},\n",
       "  {'learner': MultinomialNB(),\n",
       "   'learner__alpha': 10.0,\n",
       "   'learner__fit_prior': False,\n",
       "   'sel__k': 'all'},\n",
       "  {'learner': LinearSVC(), 'learner__C': 0.01, 'sel__k': 10000},\n",
       "  {'learner': LinearSVC(), 'learner__C': 0.01, 'sel__k': 100000},\n",
       "  {'learner': LinearSVC(), 'learner__C': 0.01, 'sel__k': 250000},\n",
       "  {'learner': LinearSVC(), 'learner__C': 0.01, 'sel__k': 500000},\n",
       "  {'learner': LinearSVC(), 'learner__C': 0.01, 'sel__k': 'all'},\n",
       "  {'learner': LinearSVC(), 'learner__C': 0.1, 'sel__k': 10000},\n",
       "  {'learner': LinearSVC(), 'learner__C': 0.1, 'sel__k': 100000},\n",
       "  {'learner': LinearSVC(), 'learner__C': 0.1, 'sel__k': 250000},\n",
       "  {'learner': LinearSVC(), 'learner__C': 0.1, 'sel__k': 500000},\n",
       "  {'learner': LinearSVC(), 'learner__C': 0.1, 'sel__k': 'all'},\n",
       "  {'learner': LinearSVC(), 'learner__C': 1, 'sel__k': 10000},\n",
       "  {'learner': LinearSVC(), 'learner__C': 1, 'sel__k': 100000},\n",
       "  {'learner': LinearSVC(), 'learner__C': 1, 'sel__k': 250000},\n",
       "  {'learner': LinearSVC(), 'learner__C': 1, 'sel__k': 500000},\n",
       "  {'learner': LinearSVC(), 'learner__C': 1, 'sel__k': 'all'},\n",
       "  {'learner': LinearSVC(), 'learner__C': 10, 'sel__k': 10000},\n",
       "  {'learner': LinearSVC(), 'learner__C': 10, 'sel__k': 100000},\n",
       "  {'learner': LinearSVC(), 'learner__C': 10, 'sel__k': 250000},\n",
       "  {'learner': LinearSVC(), 'learner__C': 10, 'sel__k': 500000},\n",
       "  {'learner': LinearSVC(), 'learner__C': 10, 'sel__k': 'all'},\n",
       "  {'learner': LinearSVC(), 'learner__C': 100, 'sel__k': 10000},\n",
       "  {'learner': LinearSVC(), 'learner__C': 100, 'sel__k': 100000},\n",
       "  {'learner': LinearSVC(), 'learner__C': 100, 'sel__k': 250000},\n",
       "  {'learner': LinearSVC(), 'learner__C': 100, 'sel__k': 500000},\n",
       "  {'learner': LinearSVC(), 'learner__C': 100, 'sel__k': 'all'},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 0.01,\n",
       "   'learner__penalty': 'l1',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 10000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 0.01,\n",
       "   'learner__penalty': 'l1',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 100000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 0.01,\n",
       "   'learner__penalty': 'l1',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 250000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 0.01,\n",
       "   'learner__penalty': 'l1',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 500000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 0.01,\n",
       "   'learner__penalty': 'l1',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 'all'},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 0.01,\n",
       "   'learner__penalty': 'l2',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 10000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 0.01,\n",
       "   'learner__penalty': 'l2',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 100000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 0.01,\n",
       "   'learner__penalty': 'l2',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 250000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 0.01,\n",
       "   'learner__penalty': 'l2',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 500000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 0.01,\n",
       "   'learner__penalty': 'l2',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 'all'},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 0.1,\n",
       "   'learner__penalty': 'l1',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 10000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 0.1,\n",
       "   'learner__penalty': 'l1',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 100000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 0.1,\n",
       "   'learner__penalty': 'l1',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 250000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 0.1,\n",
       "   'learner__penalty': 'l1',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 500000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 0.1,\n",
       "   'learner__penalty': 'l1',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 'all'},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 0.1,\n",
       "   'learner__penalty': 'l2',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 10000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 0.1,\n",
       "   'learner__penalty': 'l2',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 100000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 0.1,\n",
       "   'learner__penalty': 'l2',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 250000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 0.1,\n",
       "   'learner__penalty': 'l2',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 500000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 0.1,\n",
       "   'learner__penalty': 'l2',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 'all'},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 1,\n",
       "   'learner__penalty': 'l1',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 10000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 1,\n",
       "   'learner__penalty': 'l1',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 100000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 1,\n",
       "   'learner__penalty': 'l1',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 250000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 1,\n",
       "   'learner__penalty': 'l1',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 500000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 1,\n",
       "   'learner__penalty': 'l1',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 'all'},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 1,\n",
       "   'learner__penalty': 'l2',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 10000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 1,\n",
       "   'learner__penalty': 'l2',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 100000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 1,\n",
       "   'learner__penalty': 'l2',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 250000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 1,\n",
       "   'learner__penalty': 'l2',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 500000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 1,\n",
       "   'learner__penalty': 'l2',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 'all'},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 10,\n",
       "   'learner__penalty': 'l1',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 10000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 10,\n",
       "   'learner__penalty': 'l1',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 100000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 10,\n",
       "   'learner__penalty': 'l1',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 250000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 10,\n",
       "   'learner__penalty': 'l1',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 500000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 10,\n",
       "   'learner__penalty': 'l1',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 'all'},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 10,\n",
       "   'learner__penalty': 'l2',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 10000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 10,\n",
       "   'learner__penalty': 'l2',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 100000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 10,\n",
       "   'learner__penalty': 'l2',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 250000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 10,\n",
       "   'learner__penalty': 'l2',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 500000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 10,\n",
       "   'learner__penalty': 'l2',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 'all'},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 100,\n",
       "   'learner__penalty': 'l1',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 10000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 100,\n",
       "   'learner__penalty': 'l1',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 100000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 100,\n",
       "   'learner__penalty': 'l1',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 250000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 100,\n",
       "   'learner__penalty': 'l1',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 500000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 100,\n",
       "   'learner__penalty': 'l1',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 'all'},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 100,\n",
       "   'learner__penalty': 'l2',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 10000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 100,\n",
       "   'learner__penalty': 'l2',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 100000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 100,\n",
       "   'learner__penalty': 'l2',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 250000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 100,\n",
       "   'learner__penalty': 'l2',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 500000},\n",
       "  {'learner': LogisticRegression(C=100, solver='liblinear'),\n",
       "   'learner__C': 100,\n",
       "   'learner__penalty': 'l2',\n",
       "   'learner__solver': 'liblinear',\n",
       "   'sel__k': 'all'}],\n",
       " 'split0_test_score': array([0.72552587, 0.72245594, 0.71154065, 0.7179079 , 0.71620239,\n",
       "        0.5877203 , 0.60216032, 0.61932916, 0.68914156, 0.70949403,\n",
       "        0.72518476, 0.72609437, 0.72166003, 0.72404775, 0.72552587,\n",
       "        0.59033542, 0.6120523 , 0.62490051, 0.65571347, 0.66537806,\n",
       "        0.72063673, 0.71290506, 0.70369528, 0.69675952, 0.6941444 ,\n",
       "        0.6234224 , 0.70403638, 0.70744741, 0.70198977, 0.69653212,\n",
       "        0.70312678, 0.69243889, 0.69255259, 0.69255259, 0.69255259,\n",
       "        0.70699261, 0.69221148, 0.69198408, 0.69221148, 0.69221148,\n",
       "        0.69255259, 0.69255259, 0.69255259, 0.69255259, 0.69255259,\n",
       "        0.69221148, 0.69232518, 0.69232518, 0.69232518, 0.69232518,\n",
       "        0.69346219, 0.69266629, 0.69266629, 0.69266629, 0.69312109,\n",
       "        0.72052302, 0.72143263, 0.72029562, 0.72097783, 0.72211484,\n",
       "        0.72598067, 0.72836839, 0.73303013, 0.73541785, 0.73610006,\n",
       "        0.67913587, 0.70119386, 0.7177942 , 0.72711768, 0.73280273,\n",
       "        0.63217737, 0.67845367, 0.70471859, 0.72029562, 0.72791359,\n",
       "        0.69255259, 0.69255259, 0.69255259, 0.69255259, 0.69255259,\n",
       "        0.69255259, 0.69255259, 0.69255259, 0.69255259, 0.69255259,\n",
       "        0.69289369, 0.69289369, 0.69312109, 0.69323479, 0.6941444 ,\n",
       "        0.69346219, 0.69266629, 0.69266629, 0.69266629, 0.6935759 ,\n",
       "        0.72382035, 0.72529847, 0.72495736, 0.72609437, 0.72666288,\n",
       "        0.71904491, 0.72154633, 0.71984082, 0.72006822, 0.72052302,\n",
       "        0.69880614, 0.70744741, 0.71233655, 0.71495168, 0.72120523,\n",
       "        0.72347925, 0.72552587, 0.729278  , 0.73166572, 0.73200682,\n",
       "        0.6235361 , 0.67220011, 0.682888  , 0.68971006, 0.69801023,\n",
       "        0.6823195 , 0.71449687, 0.72529847, 0.73246163, 0.73325753]),\n",
       " 'split1_test_score': array([0.72256964, 0.72120523, 0.71324616, 0.71938601, 0.71824901,\n",
       "        0.60579875, 0.61421262, 0.63217737, 0.68368391, 0.71472428,\n",
       "        0.72245594, 0.72472996, 0.72268334, 0.72120523, 0.72177374,\n",
       "        0.60989198, 0.62569642, 0.6354747 , 0.6584423 , 0.66810688,\n",
       "        0.72086413, 0.71381467, 0.70301308, 0.69664582, 0.6938033 ,\n",
       "        0.64377487, 0.7063104 , 0.70835702, 0.69926094, 0.69619102,\n",
       "        0.70278567, 0.69255259, 0.69255259, 0.69255259, 0.69255259,\n",
       "        0.71028994, 0.69198408, 0.69187038, 0.69209778, 0.69209778,\n",
       "        0.69255259, 0.69255259, 0.69255259, 0.69255259, 0.69255259,\n",
       "        0.69198408, 0.69198408, 0.69198408, 0.69209778, 0.69209778,\n",
       "        0.6936896 , 0.69277999, 0.69277999, 0.69289369, 0.69323479,\n",
       "        0.72131893, 0.71927231, 0.7177942 , 0.717112  , 0.71904491,\n",
       "        0.72609437, 0.72689028, 0.73018761, 0.73234792, 0.73428084,\n",
       "        0.68436612, 0.70255827, 0.71949972, 0.7295054 , 0.73325753,\n",
       "        0.6349062 , 0.67981808, 0.7064241 , 0.71915861, 0.7291643 ,\n",
       "        0.69255259, 0.69255259, 0.69255259, 0.69255259, 0.69255259,\n",
       "        0.69255259, 0.69255259, 0.69255259, 0.69255259, 0.69255259,\n",
       "        0.69346219, 0.69277999, 0.69277999, 0.69300739, 0.6938033 ,\n",
       "        0.69312109, 0.69277999, 0.69277999, 0.69277999, 0.69277999,\n",
       "        0.72166003, 0.72336555, 0.72393405, 0.72427516, 0.72632177,\n",
       "        0.71972712, 0.71949972, 0.7175668 , 0.71699829, 0.71847641,\n",
       "        0.70437749, 0.71188175, 0.71461057, 0.7180216 , 0.72313815,\n",
       "        0.72450256, 0.7297328 , 0.73132462, 0.73257533, 0.73473565,\n",
       "        0.62649233, 0.67333712, 0.68368391, 0.68993746, 0.69664582,\n",
       "        0.68800455, 0.7181353 , 0.72734508, 0.73291643, 0.73587265]),\n",
       " 'split2_test_score': array([0.72256964, 0.72109153, 0.71586128, 0.71984082, 0.71893121,\n",
       "        0.6005685 , 0.60795907, 0.63047186, 0.68720864, 0.71472428,\n",
       "        0.72222854, 0.72563957, 0.72256964, 0.72268334, 0.72086413,\n",
       "        0.60284252, 0.62274019, 0.63649801, 0.65662308, 0.66583286,\n",
       "        0.71995452, 0.71392837, 0.70369528, 0.69766913, 0.69562251,\n",
       "        0.63718022, 0.7058556 , 0.70881182, 0.70210347, 0.69880614,\n",
       "        0.70255827, 0.69255259, 0.69255259, 0.69255259, 0.69255259,\n",
       "        0.7055145 , 0.69232518, 0.69221148, 0.69232518, 0.69232518,\n",
       "        0.69255259, 0.69255259, 0.69255259, 0.69255259, 0.69255259,\n",
       "        0.69209778, 0.69232518, 0.69232518, 0.69232518, 0.69232518,\n",
       "        0.6940307 , 0.69334849, 0.69300739, 0.69312109, 0.69346219,\n",
       "        0.71915861, 0.72006822, 0.71938601, 0.71938601, 0.71995452,\n",
       "        0.71915861, 0.72882319, 0.73109721, 0.73041501, 0.73291643,\n",
       "        0.68038658, 0.70005685, 0.7172257 , 0.72382035, 0.73064241,\n",
       "        0.62990335, 0.67367823, 0.7056282 , 0.71529278, 0.72598067,\n",
       "        0.69255259, 0.69255259, 0.69255259, 0.69255259, 0.69255259,\n",
       "        0.69255259, 0.69255259, 0.69255259, 0.69255259, 0.69255259,\n",
       "        0.69346219, 0.69323479, 0.69312109, 0.69346219, 0.6942581 ,\n",
       "        0.693917  , 0.69312109, 0.69300739, 0.69300739, 0.6938033 ,\n",
       "        0.72075043, 0.72393405, 0.72404775, 0.72450256, 0.72484366,\n",
       "        0.71915861, 0.71961342, 0.71961342, 0.71870381, 0.71859011,\n",
       "        0.69550881, 0.70505969, 0.70949403, 0.71370097, 0.7179079 ,\n",
       "        0.71631609, 0.72689028, 0.72870949, 0.7299602 , 0.73212052,\n",
       "        0.62683343, 0.67310972, 0.682888  , 0.68675384, 0.69562251,\n",
       "        0.68561683, 0.71654349, 0.72632177, 0.729278  , 0.73462194]),\n",
       " 'mean_test_score': array([0.72355505, 0.72158423, 0.71354937, 0.71904491, 0.7177942 ,\n",
       "        0.59802918, 0.60811067, 0.62732613, 0.68667804, 0.71298086,\n",
       "        0.72328975, 0.72548797, 0.72230434, 0.72264544, 0.72272124,\n",
       "        0.60102331, 0.62016297, 0.63229107, 0.65692628, 0.66643926,\n",
       "        0.72048512, 0.71354937, 0.70346788, 0.69702482, 0.6945234 ,\n",
       "        0.6347925 , 0.7054008 , 0.70820542, 0.70111806, 0.69717643,\n",
       "        0.70282357, 0.69251469, 0.69255259, 0.69255259, 0.69255259,\n",
       "        0.70759901, 0.69217358, 0.69202198, 0.69221148, 0.69221148,\n",
       "        0.69255259, 0.69255259, 0.69255259, 0.69255259, 0.69255259,\n",
       "        0.69209778, 0.69221148, 0.69221148, 0.69224938, 0.69224938,\n",
       "        0.6937275 , 0.69293159, 0.69281789, 0.69289369, 0.69327269,\n",
       "        0.72033352, 0.72025772, 0.71915861, 0.71915861, 0.72037142,\n",
       "        0.72374455, 0.72802729, 0.73143832, 0.73272693, 0.73443244,\n",
       "        0.68129619, 0.70126966, 0.7181732 , 0.72681448, 0.73223422,\n",
       "        0.63232897, 0.67731666, 0.7055903 , 0.71824901, 0.72768619,\n",
       "        0.69255259, 0.69255259, 0.69255259, 0.69255259, 0.69255259,\n",
       "        0.69255259, 0.69255259, 0.69255259, 0.69255259, 0.69255259,\n",
       "        0.69327269, 0.69296949, 0.69300739, 0.69323479, 0.6940686 ,\n",
       "        0.69350009, 0.69285579, 0.69281789, 0.69281789, 0.69338639,\n",
       "        0.72207694, 0.72419936, 0.72431306, 0.72495736, 0.72594277,\n",
       "        0.71931021, 0.72021982, 0.71900701, 0.71859011, 0.71919651,\n",
       "        0.69956415, 0.70812962, 0.71214705, 0.71555808, 0.72075043,\n",
       "        0.72143263, 0.72738298, 0.7297707 , 0.73140042, 0.73295433,\n",
       "        0.62562062, 0.67288232, 0.68315331, 0.68880045, 0.69675952,\n",
       "        0.68531363, 0.71639189, 0.72632177, 0.73155202, 0.73458404]),\n",
       " 'std_test_score': array([1.39357789e-03, 6.18135548e-04, 1.77687446e-03, 8.25148420e-04,\n",
       "        1.15952689e-03, 7.59577678e-03, 4.92149948e-03, 5.69741564e-03,\n",
       "        2.25944416e-03, 2.46556088e-03, 1.34319084e-03, 5.67240081e-04,\n",
       "        4.57951335e-04, 1.16076504e-03, 2.01763603e-03, 8.08690214e-03,\n",
       "        5.86071837e-03, 5.24258869e-03, 1.13448016e-03, 1.19370974e-03,\n",
       "        3.86508964e-04, 4.57951335e-04, 3.21594898e-04, 4.57951335e-04,\n",
       "        7.89564778e-04, 8.47866717e-03, 9.82488641e-04, 5.67240081e-04,\n",
       "        1.31399930e-03, 1.16076504e-03, 2.33633277e-04, 5.35991496e-05,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.99616359e-03,\n",
       "        1.41810020e-04, 1.41810020e-04, 9.28364504e-05, 9.28364504e-05,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 9.28364504e-05, 1.60797449e-04, 1.60797449e-04,\n",
       "        1.07198299e-04, 1.07198299e-04, 2.33633277e-04, 2.98427435e-04,\n",
       "        1.41810020e-04, 1.85672901e-04, 1.41810020e-04, 8.92067637e-04,\n",
       "        8.92067637e-04, 1.03378296e-03, 1.58638996e-03, 1.28749576e-03,\n",
       "        3.24308076e-03, 8.25148420e-04, 1.18525645e-03, 2.05990953e-03,\n",
       "        1.30412360e-03, 2.23000806e-03, 1.02260660e-03, 9.66272411e-04,\n",
       "        2.33079278e-03, 1.14079340e-03, 2.04521320e-03, 2.63236769e-03,\n",
       "        6.96788945e-04, 2.14128435e-03, 1.30961929e-03, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.67995748e-04, 1.93254482e-04, 1.60797449e-04,\n",
       "        1.85672901e-04, 1.93254482e-04, 3.26030899e-04, 1.93254482e-04,\n",
       "        1.41810020e-04, 1.41810020e-04, 4.38727948e-04, 1.28749576e-03,\n",
       "        8.11102314e-04, 4.57951335e-04, 8.09329411e-04, 7.89564778e-04,\n",
       "        2.98427435e-04, 9.39132969e-04, 1.02260660e-03, 1.25586822e-03,\n",
       "        9.39132969e-04, 3.66008024e-03, 2.82656120e-03, 2.09311370e-03,\n",
       "        1.81526322e-03, 2.15932076e-03, 3.64198235e-03, 1.75245450e-03,\n",
       "        1.12302690e-03, 1.08397572e-03, 1.26043503e-03, 1.48054008e-03,\n",
       "        4.91244321e-04, 3.75194047e-04, 1.45015171e-03, 9.78092697e-04,\n",
       "        2.33079278e-03, 1.48924637e-03, 8.35528053e-04, 1.61865882e-03,\n",
       "        1.06795549e-03]),\n",
       " 'rank_test_score': array([ 21,  27,  48,  39,  44, 125, 123, 120, 109,  49,  22,  16,  25,\n",
       "         24,  23, 124, 122, 119, 116, 115,  30,  47,  56,  62,  64, 117,\n",
       "         55,  51,  59,  61,  57,  98,  80,  80,  80,  53, 105, 107, 101,\n",
       "        101,  80,  80,  80,  80,  80, 106, 101, 101,  99,  99,  66,  74,\n",
       "         77,  75,  69,  32,  33,  37,  37,  31,  20,  10,   7,   4,   2,\n",
       "        112,  58,  43,  13,   5, 118, 113,  54,  42,  11,  80,  80,  80,\n",
       "         80,  80,  80,  80,  80,  80,  80,  69,  73,  72,  71,  65,  67,\n",
       "         76,  77,  77,  68,  26,  19,  18,  17,  15,  35,  34,  40,  41,\n",
       "         36,  60,  52,  50,  46,  29,  28,  12,   9,   8,   3, 121, 114,\n",
       "        111, 108,  63, 110,  45,  14,   6,   1])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('sel',\n",
       "                 SelectKBest(k='all',\n",
       "                             score_func=<function chi2 at 0x0000025C6868DD30>)),\n",
       "                ('tfidf', TfidfTransformer()),\n",
       "                ('learner', LogisticRegression(C=100, solver='liblinear'))])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_predictions = opt_search.best_estimator_.predict(X_test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   arancione       0.35      0.05      0.09       787\n",
      "      giallo       0.41      0.17      0.24      2078\n",
      "       rosso       0.79      0.63      0.70       612\n",
      "       verde       0.77      0.96      0.85      7831\n",
      "\n",
      "    accuracy                           0.74     11308\n",
      "   macro avg       0.58      0.45      0.47     11308\n",
      "weighted avg       0.67      0.74      0.68     11308\n",
      "\n",
      "Confusion matrix:\n",
      "[[  40  194   78  475]\n",
      " [  36  354   18 1670]\n",
      " [  21   65  388  138]\n",
      " [  17  253    9 7552]]\n"
     ]
    }
   ],
   "source": [
    "print('Classification report:')\n",
    "print(classification_report(y_test, opt_predictions))\n",
    "print('Confusion matrix:')\n",
    "opt_cm = confusion_matrix(y_test, opt_predictions)\n",
    "print(opt_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomSearchCV on restrict parameter range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_search_space = [{'learner__C': np.arange(50, 100, 10)}]\n",
    "\n",
    "opt_pipeline = Pipeline([\n",
    "#    ('vect', CountVectorizer()),  # feature extraction\n",
    "    ('sel', SelectKBest(chi2, k='all')),  # feature selection\n",
    "    ('tfidf', TfidfTransformer()),  # weighting\n",
    "    ('learner', LogisticRegression(solver = 'liblinear', penalty = 'l2'))  # learning algorithm\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chiar\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 5 is smaller than n_iter=10. Running 5 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chiar\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "opt_rsearch = RandomizedSearchCV(opt_pipeline, \n",
    "                                rand_search_space,\n",
    "                                cv=3, n_jobs = 4, verbose=True).fit(X_train_tok, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learner__C': 90}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_rsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([401.19844874, 417.94750245, 418.57340789, 408.83339747,\n",
       "        255.76138902]),\n",
       " 'std_fit_time': array([ 2.81196708, 10.51268024,  7.54339896,  2.74024478,  4.20751885]),\n",
       " 'mean_score_time': array([3.84658225, 4.75797327, 3.8578709 , 4.18003241, 2.39357313]),\n",
       " 'std_score_time': array([0.76216091, 1.22149192, 0.42520472, 0.26125259, 0.84954168]),\n",
       " 'param_learner__C': masked_array(data=[50, 60, 70, 80, 90],\n",
       "              mask=[False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'learner__C': 50},\n",
       "  {'learner__C': 60},\n",
       "  {'learner__C': 70},\n",
       "  {'learner__C': 80},\n",
       "  {'learner__C': 90}],\n",
       " 'split0_test_score': array([0.73291643, 0.73291643, 0.73268903, 0.73291643, 0.73348493]),\n",
       " 'split1_test_score': array([0.73473565, 0.73530415, 0.73530415, 0.73564525, 0.73598636]),\n",
       " 'split2_test_score': array([0.73382604, 0.73393974, 0.73473565, 0.73462194, 0.73473565]),\n",
       " 'mean_test_score': array([0.73382604, 0.73405344, 0.73424294, 0.73439454, 0.73473565]),\n",
       " 'std_test_score': array([0.00074269, 0.00097809, 0.00112303, 0.00112558, 0.0010212 ]),\n",
       " 'rank_test_score': array([5, 4, 3, 2, 1])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_rsearch.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('sel',\n",
       "                 SelectKBest(k='all',\n",
       "                             score_func=<function chi2 at 0x0000025C6868DD30>)),\n",
       "                ('tfidf', TfidfTransformer()),\n",
       "                ('learner', LogisticRegression(C=90, solver='liblinear'))])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_rsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_predictions = opt_rsearch.best_estimator_.predict(X_test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   arancione       0.36      0.05      0.09       787\n",
      "      giallo       0.41      0.17      0.24      2078\n",
      "       rosso       0.79      0.63      0.70       612\n",
      "       verde       0.77      0.96      0.86      7831\n",
      "\n",
      "    accuracy                           0.74     11308\n",
      "   macro avg       0.58      0.45      0.47     11308\n",
      "weighted avg       0.67      0.74      0.68     11308\n",
      "\n",
      "Confusion matrix:\n",
      "[[  40  194   78  475]\n",
      " [  34  355   17 1672]\n",
      " [  20   67  386  139]\n",
      " [  17  250    9 7555]]\n"
     ]
    }
   ],
   "source": [
    "print('Classification report:')\n",
    "print(classification_report(y_test, opt_predictions))\n",
    "print('Confusion matrix:')\n",
    "opt_cm = confusion_matrix(y_test, opt_predictions)\n",
    "print(opt_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification verde vs rosso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bdf.Rating.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "verde    26104\n",
       "rosso     2039\n",
       "Name: Rating, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdf.Rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "btarget = bdf[['Rating']]\n",
    "del bdf['Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split on train-test \n",
    "x_train_bin, x_test_bin, y_train_bin, y_test_bin = train_test_split(bdf, btarget, test_size=0.30, random_state=42, stratify=btarget, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 19700\n",
      "Test set size: 8443\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train set size: {len(x_train_bin)}\\nTest set size: {len(x_test_bin)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vect = CountVectorizer(analyzer=spacy_nlp_tokenizer, min_df=5)  \n",
    "#reset_counter()\n",
    "#X_train_tok = vect.fit_transform(x_train.Racconto_Text_Only)\n",
    "#reset_counter()\n",
    "#X_test_tok = vect.transform(x_test.Racconto_Text_Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(path+r'\\Rating\\x_train_tok_verdirossi.pkl','wb') as outfile:\n",
    "#    pickle.dump(X_train_tok,outfile)\n",
    "#with open(path+r'\\Rating\\x_test_tok_verdirossi.pkl','wb') as outfile:\n",
    "#    pickle.dump(X_test_tok,outfile)\n",
    "#with open(path+r'\\Rating\\vect_verdirossi.pkl','wb') as outfile:\n",
    "#    pickle.dump(vect,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "btrain_file = open(path+r\"\\Rating\\x_train_tok_verdirossi.pkl\", \"rb\")\n",
    "x_train_bin_tok = pickle.load(btrain_file)\n",
    "\n",
    "btest_file = open(path+r\"\\Rating\\x_test_tok_verdirossi.pkl\", \"rb\")\n",
    "x_test_bin_tok = pickle.load(btest_file)\n",
    "\n",
    "bvect_file = open(path+r\"\\Rating\\vect_verdirossi.pkl\", \"rb\")\n",
    "bvect = pickle.load(bvect_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lunghezza del vocabolario: 629377\n"
     ]
    }
   ],
   "source": [
    "print(f\"Lunghezza del vocabolario: {len(bvect.vocabulary_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_counter()\n",
    "pipeline.fit(x_train_bin_tok, y_train_bin.values.ravel())\n",
    "\n",
    "reset_counter()\n",
    "ratingb_predictions = pipeline.predict(x_test_bin_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       rosso       0.97      0.65      0.78       612\n",
      "       verde       0.97      1.00      0.99      7831\n",
      "\n",
      "    accuracy                           0.97      8443\n",
      "   macro avg       0.97      0.82      0.88      8443\n",
      "weighted avg       0.97      0.97      0.97      8443\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 396  216]\n",
      " [  12 7819]]\n"
     ]
    }
   ],
   "source": [
    "print('Classification report:')\n",
    "print(classification_report(y_test_bin, ratingb_predictions))\n",
    "print('Confusion matrix:')\n",
    "ratingb_cm = confusion_matrix(y_test_bin, ratingb_predictions)\n",
    "print(ratingb_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsearch_space = [{'sel__k': [10000, 100000, 250000, 'all'],\n",
    "                 'learner':[MultinomialNB(class_prior = [.2, .1])],\n",
    "                 'learner__alpha': [1e-3, 1e-2, 0.1, 1.0, 10.0],\n",
    "                 'learner__fit_prior':[True, False]}, \n",
    "                {'sel__k': [10000, 100000, 250000, 'all'],\n",
    "                 'learner':[LinearSVC(class_weight = {'rosso':2, 'verde': 1})],\n",
    "                 'learner__C': [0.01, 0.1, 1, 10, 100]},\n",
    "                {'sel__k': [10000, 100000, 250000, 'all'], \n",
    "                 'learner':[LogisticRegression(class_weight = {'rosso':2, 'verde': 1})],\n",
    "                 'learner__solver':[\"liblinear\"],\n",
    "                 'learner__penalty' : ['l1', 'l2'],\n",
    "                 'learner__C': [0.01, 0.1, 1, 10, 100]} \n",
    "               ]\n",
    "\n",
    "brating_opt_pipeline = Pipeline([\n",
    "#    ('vect', CountVectorizer()),  # feature extraction\n",
    "    ('sel', SelectKBest(chi2)),  # feature selection\n",
    "    ('tfidf', TfidfTransformer()),  # weighting\n",
    "    ('learner', LinearSVC(class_weight={'rosso':2,'verde':1}))  # learning algorithm\n",
    "])\n",
    "\n",
    "scoring = make_scorer(f1_score, greater_is_better=True, pos_label='rosso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    }
   ],
   "source": [
    "brating_opt_search = GridSearchCV(brating_opt_pipeline, \n",
    "                                 bsearch_space,\n",
    "                                 scoring = scoring,\n",
    "                                 cv=3, n_jobs = 4, verbose=True).fit(x_train_bin_tok, y_train_bin.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('sel',\n",
       "                 SelectKBest(k='all',\n",
       "                             score_func=<function chi2 at 0x000001B152FECDC0>)),\n",
       "                ('tfidf', TfidfTransformer()),\n",
       "                ('learner',\n",
       "                 LogisticRegression(C=100,\n",
       "                                    class_weight={'rosso': 2, 'verde': 1},\n",
       "                                    penalty='l1', solver='liblinear'))])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brating_opt_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "brating_opt_predictions = brating_opt_search.best_estimator_.predict(x_test_bin_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       rosso       0.87      0.75      0.81       612\n",
      "       verde       0.98      0.99      0.99      7831\n",
      "\n",
      "    accuracy                           0.97      8443\n",
      "   macro avg       0.92      0.87      0.90      8443\n",
      "weighted avg       0.97      0.97      0.97      8443\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 459  153]\n",
      " [  69 7762]]\n"
     ]
    }
   ],
   "source": [
    "print('Classification report:')\n",
    "print(classification_report(y_test_bin, brating_opt_predictions))\n",
    "print('Confusion matrix:')\n",
    "brating_opt_cm = confusion_matrix(y_test_bin, brating_opt_predictions)\n",
    "print(brating_opt_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_opt_pipeline = Pipeline([\n",
    "    ('sel', SelectKBest(k='all', score_func=chi2)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('learner', LogisticRegression(C=10,\n",
    "                                    class_weight={'rosso': 2, 'verde': 1},\n",
    "                                    penalty='l1', solver='liblinear'))\n",
    "                                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('sel',\n",
       "                 SelectKBest(k='all',\n",
       "                             score_func=<function chi2 at 0x000001B152FECDC0>)),\n",
       "                ('tfidf', TfidfTransformer()),\n",
       "                ('learner',\n",
       "                 LogisticRegression(C=10, class_weight={'rosso': 2, 'verde': 1},\n",
       "                                    penalty='l1', solver='liblinear'))])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_opt_pipeline.fit(x_train_bin_tok, y_train_bin.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_tokenizer = bvect\n",
    "rating_selector = model_opt_pipeline.named_steps['sel']\n",
    "rating_classifier = model_opt_pipeline.named_steps['learner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "629377"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_feature_names = rating_tokenizer.get_feature_names()\n",
    "rating_feats_w_score = list()\n",
    "for index,(selected,score) in enumerate(zip(rating_selector.get_support(),rating_selector.scores_)):\n",
    "    rating_feats_w_score.append((score,selected,rating_feature_names[index]))\n",
    "rating_feats_w_score = sorted(rating_feats_w_score)\n",
    "len(rating_feats_w_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1133"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_feats_w_classifier_weight = list()\n",
    "for index,weight in enumerate(rating_selector.inverse_transform(rating_classifier.coef_)[0]):\n",
    "    if weight!=0:\n",
    "        rating_feats_w_classifier_weight.append((weight,rating_feature_names[index]))\n",
    "rating_feats_w_classifier_weight = sorted(rating_feats_w_classifier_weight)\n",
    "len(rating_feats_w_classifier_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6.599432760260515, 'BI_LEMMA_Harry_LEMMA_guardare'),\n",
       " (6.675969780552663, 'BI_LEMMA_»_LEMMA_.'),\n",
       " (6.759003311775778, 'LEMMA_comprare'),\n",
       " (6.893761624527026, 'BI_LEMMA_,_LEMMA_il'),\n",
       " (6.966431248992194, 'LEMMA_frase'),\n",
       " (7.002032500463452, 'BI_LEMMA_._LEMMA_-Ma'),\n",
       " (7.024506696855564, 'LEMMA_correre'),\n",
       " (7.033818242725572, 'LEMMA_braccio'),\n",
       " (7.110717564896361, 'LEMMA_credere'),\n",
       " (7.138652916555201, 'LEMMA_dalla'),\n",
       " (7.147248266442415, 'LEMMA_piccolo'),\n",
       " (7.200377332990953, 'LEMMA_moglie'),\n",
       " (7.314298430160042, 'BI_LEMMA_Draco_LEMMA_Malfoy'),\n",
       " (7.332133808529103, 'LEMMA_profumare'),\n",
       " (7.338974041739393, 'BI_LEMMA_Potter_LEMMA_e'),\n",
       " (7.344273993775025, 'BI_LEMMA_._LEMMA_girare'),\n",
       " (7.47154516037599, 'LEMMA_terra'),\n",
       " (7.478380779626793, 'LEMMA_sedia'),\n",
       " (7.505906300702909, 'LEMMA_circondare'),\n",
       " (7.666709077259793, 'LEMMA_diverso'),\n",
       " (7.667826179540596, 'LEMMA_Natale'),\n",
       " (7.689570392529787, 'LEMMA_scrivere'),\n",
       " (7.750202927518971, 'LEMMA_raccontare'),\n",
       " (7.807724856867375, 'BI_LEMMA_sposare_LEMMA_,'),\n",
       " (7.843039990753961, 'TRI_LEMMA_!_LEMMA_”_LEMMA_dire'),\n",
       " (7.996965467572113, 'LEMMA_soprattutto'),\n",
       " (8.127488699076013, 'LEMMA_Andromeda'),\n",
       " (8.134877046562579, 'LEMMA_cogliere'),\n",
       " (8.184047129944746, 'LEMMA_Lupin'),\n",
       " (8.216819071709645, 'BI_LEMMA_genitore_LEMMA_,'),\n",
       " (8.244535842479838, 'LEMMA_ridere'),\n",
       " (8.26981636646246, 'LEMMA_meritare'),\n",
       " (8.364286721917157, 'LEMMA_stupido'),\n",
       " (8.45615498012339, 'BI_LEMMA_l’_LEMMA_ultimo'),\n",
       " (8.639495660190095, 'LEMMA_pentire'),\n",
       " (8.875300788867118, 'LEMMA_lago'),\n",
       " (8.905130832858754, 'LEMMA_esistenza'),\n",
       " (8.981375138359498, 'LEMMA_dottore'),\n",
       " (9.00962714610169, 'LEMMA_sperare'),\n",
       " (9.087615518682012, 'LEMMA_pagina'),\n",
       " (9.366348302374721, 'LEMMA_manica'),\n",
       " (9.58169139113242, 'LEMMA_illuminare'),\n",
       " (9.668910504315392, 'LEMMA_!'),\n",
       " (9.729503275822337, 'LEMMA_capire'),\n",
       " (9.924506802581591, 'LEMMA_diario'),\n",
       " (9.955217289474842, 'LEMMA_:)'),\n",
       " (9.959528316849605, 'BI_LEMMA_Silente_LEMMA_,'),\n",
       " (10.099589859313749, 'LEMMA_cuore'),\n",
       " (10.257834829277881, 'LEMMA_sospirare'),\n",
       " (10.444098563939132, 'LEMMA_:'),\n",
       " (10.482683026952166, 'LEMMA_Marlene'),\n",
       " (10.516238734901897, 'BI_LEMMA_._LEMMA_!'),\n",
       " (10.5280829419369, 'TRI_LEMMA_:_LEMMA_<_LEMMA_<'),\n",
       " (10.816965093866262, 'BI_LEMMA_“_LEMMA_E'),\n",
       " (10.952238324926801, 'LEMMA_socchiudere'),\n",
       " (11.044292928385033, 'LEMMA_McGranitt'),\n",
       " (11.139900472283424, 'LEMMA_vero'),\n",
       " (11.27353348340416, 'LEMMA_Fleur'),\n",
       " (11.284062632347862, 'LEMMA_nuvolo'),\n",
       " (11.411686901220477, 'LEMMA_oggetto'),\n",
       " (11.512717696735923, 'LEMMA_strano'),\n",
       " (11.78581116594594, 'LEMMA_calore'),\n",
       " (11.849977984100766, 'BI_LEMMA_._LEMMA_<'),\n",
       " (12.232685942828766, 'BI_LEMMA_._LEMMA_sangue'),\n",
       " (12.414669443765302, 'LEMMA_tazza'),\n",
       " (12.752123436998902, 'LEMMA_accanto'),\n",
       " (12.767737933050006, 'BI_LEMMA_forte_LEMMA_e'),\n",
       " (12.831258003157789, 'LEMMA_ispirazione'),\n",
       " (12.955629068031888, 'LEMMA_Hagrid'),\n",
       " (12.971905327866697, 'LEMMA_tasca'),\n",
       " (13.220884218956334, 'BI_LEMMA_baciare_LEMMA_.'),\n",
       " (13.266554847416673, 'BI_LEMMA_,_LEMMA_Pansy'),\n",
       " (13.957018729894047, 'BI_LEMMA_“_LEMMA_...'),\n",
       " (14.064342730616893, 'LEMMA_ballare'),\n",
       " (14.09004413632459, 'LEMMA_['),\n",
       " (14.168475082915263, 'LEMMA_freddare'),\n",
       " (14.52956542583982, 'LEMMA_musico'),\n",
       " (14.55680069052313, 'TRI_LEMMA_,_LEMMA_l_LEMMA_’'),\n",
       " (14.557842629629661, 'BI_LEMMA_Harry_LEMMA_.'),\n",
       " (14.607612549118135, 'LEMMA_orgoglioso'),\n",
       " (14.764487359184166, 'LEMMA_metro'),\n",
       " (15.030109015064747, 'BI_LEMMA_;_LEMMA_,'),\n",
       " (15.304851974189955, 'BI_LEMMA_figliare_LEMMA_Lord'),\n",
       " (15.382627953053413, 'LEMMA_fiancare'),\n",
       " (15.556495677392672, 'LEMMA_recensire'),\n",
       " (17.03993809960719, 'LEMMA_saltare'),\n",
       " (18.001961351340302, 'LEMMA_stancare'),\n",
       " (18.71304973975344, 'BI_LEMMA_Potter_LEMMA_.'),\n",
       " (18.87688649690552, 'LEMMA_infantile'),\n",
       " (19.24445842521333, 'BI_LEMMA_andare_LEMMA_.'),\n",
       " (19.46761996748738, 'LEMMA_imbarazzare'),\n",
       " (20.716479411192605, 'BI_LEMMA_,_LEMMA_credere'),\n",
       " (20.764442884798644, 'LEMMA_cielo'),\n",
       " (20.997765819548878, 'LEMMA_borsa'),\n",
       " (21.60828298474579, 'LEMMA_Oh'),\n",
       " (22.33404743438953, 'BI_LEMMA_essere_LEMMA_e'),\n",
       " (22.638304842730385, 'LEMMA_sorridere'),\n",
       " (23.52654063375062, 'LEMMA_superficie'),\n",
       " (23.72655605682807, 'LEMMA_Narcissa'),\n",
       " (29.251091940390772, 'BI_LEMMA_Lily_LEMMA_.')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_feats_w_classifier_weight[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-135.09236162285455, 'LEMMA_erezione'),\n",
       " (-105.16557192033974, 'LEMMA_orgasmo'),\n",
       " (-83.23114641983798, 'LEMMA_capezzolo'),\n",
       " (-83.2063692523003, 'LEMMA_Prologo'),\n",
       " (-75.01655053676055, 'LEMMA_CAPITOLO'),\n",
       " (-62.363372882716085, 'LEMMA_seno'),\n",
       " (-59.73900411995162, 'LEMMA_gemito'),\n",
       " (-58.641602320835915, 'LEMMA_spinto'),\n",
       " (-55.85402200041977, 'LEMMA_gemere'),\n",
       " (-48.29723116107557, 'BI_LEMMA_gamba_LEMMA_,'),\n",
       " (-45.83517869735735, 'LEMMA_copertura'),\n",
       " (-43.6657383982502, 'BI_LEMMA_e_LEMMA_famoso'),\n",
       " (-43.44585072368025, 'LEMMA_ansimare'),\n",
       " (-43.350431126616016, 'LEMMA_pantalone'),\n",
       " (-42.3423531484445, 'LEMMA_Capitolo'),\n",
       " (-40.895323471138006, 'LEMMA_coscia'),\n",
       " (-40.70734154019333, 'LEMMA_capitolare'),\n",
       " (-39.31468833514485, 'LEMMA_smistamento'),\n",
       " (-37.41900372766273, 'LEMMA_spogliare'),\n",
       " (-36.61677448392007, 'LEMMA_sesso'),\n",
       " (-36.2878458370765, 'LEMMA_-Lei'),\n",
       " (-35.98702539992929, 'BI_LEMMA_:_LEMMA_1/1'),\n",
       " (-34.79834839994038, 'LEMMA_eccitazione'),\n",
       " (-34.419722707489505, 'BI_LEMMA_scena_LEMMA_sesso'),\n",
       " (-34.2241344352652, 'LEMMA_Frodo'),\n",
       " (-33.95010926952113, 'LEMMA_nudo'),\n",
       " (-33.22984373132958, 'BI_LEMMA_Ormai_LEMMA_l’'),\n",
       " (-33.1002468012665, 'LEMMA_mutandina'),\n",
       " (-31.8463156196787, 'LEMMA_cazzo'),\n",
       " (-31.555939815369904, 'BI_LEMMA_David_LEMMA_.'),\n",
       " (-31.24432033743446, 'BI_LEMMA_baciare_LEMMA_mano'),\n",
       " (-30.954825467859994, 'LEMMA_corpo'),\n",
       " (-30.68905093087625, 'BI_LEMMA_pericoloso_LEMMA_,'),\n",
       " (-30.579051720991586, 'LEMMA_spingere'),\n",
       " (-29.663186148675987, 'LEMMA_prologo'),\n",
       " (-29.529652094938456, 'BI_LEMMA_provare_LEMMA_dolore'),\n",
       " (-29.242598691768826, 'LEMMA_scendere'),\n",
       " (-29.0906951147694, 'BI_LEMMA_accarezzandole_LEMMA_il'),\n",
       " (-28.325640184623833, 'BI_LEMMA_essa_LEMMA_,'),\n",
       " (-27.78427198563786, 'TRI_LEMMA_!_LEMMA_odiare_LEMMA_!'),\n",
       " (-27.57093656008567, 'LEMMA_Granger.-'),\n",
       " (-27.256762702538158, 'LEMMA_Brasile'),\n",
       " (-27.199901258270433, 'BI_LEMMA_,_LEMMA_sopratutto'),\n",
       " (-27.085239634010552, 'BI_LEMMA_copyright_LEMMA_.'),\n",
       " (-26.45750374956752, 'BI_LEMMA_capello_LEMMA_lungo'),\n",
       " (-26.39723885221741, 'BI_LEMMA_nella_LEMMA_account'),\n",
       " (-26.29159124253127, 'TRI_LEMMA_”_LEMMA_dire_LEMMA_Draco'),\n",
       " (-26.122387537397998, 'LEMMA_studiato'),\n",
       " (-25.897302667615598, 'LEMMA_carezza'),\n",
       " (-25.713867398148796, 'BI_LEMMA_il_LEMMA_pantalone'),\n",
       " (-25.613798503396804, 'LEMMA_infastidire'),\n",
       " (-25.483636299818613, 'LEMMA_intimità'),\n",
       " (-25.37236367783667, 'BI_LEMMA_,_LEMMA_fisicamente'),\n",
       " (-25.10362833259876, 'LEMMA_fottere'),\n",
       " (-24.95033412783602, 'LEMMA_Lasciai'),\n",
       " (-24.811537813288012, 'LEMMA_verginità'),\n",
       " (-24.75091237514829, 'TRI_LEMMA_-Sì_LEMMA_,_LEMMA_Harry'),\n",
       " (-24.459116158891963, 'LEMMA_lemon'),\n",
       " (-24.12823443319463, 'LEMMA_bocca'),\n",
       " (-24.107815557356947, 'LEMMA_disperatamente'),\n",
       " (-24.086952358785556, 'BI_LEMMA_e_LEMMA_Granger'),\n",
       " (-24.03519018868685, \"LEMMA_we'\"),\n",
       " (-24.027334924485658, 'BI_LEMMA_:_LEMMA_('),\n",
       " (-23.65104843406517, 'LEMMA_fianco'),\n",
       " (-23.409820401356956, 'LEMMA_sadomaso'),\n",
       " (-23.313274292323577, 'BI_LEMMA_\"_LEMMA_nonna'),\n",
       " (-23.04677181305587, 'LEMMA_perverso'),\n",
       " (-22.962787794509282, 'BI_LEMMA_-Mi_LEMMA_mancato'),\n",
       " (-22.533420378512815, 'BI_LEMMA_–_LEMMA_rispose'),\n",
       " (-22.477803070955684, 'BI_LEMMA_sofferenza_LEMMA_,'),\n",
       " (-22.44917847824222, 'LEMMA_penetrare'),\n",
       " (-22.37848794293761, 'BI_LEMMA_imprimere_LEMMA_mentire'),\n",
       " (-22.175518174400302, 'LEMMA_movimento'),\n",
       " (-21.81473952309438, 'LEMMA_lingua'),\n",
       " (-21.611968090064476, 'LEMMA_collare'),\n",
       " (-21.48652625919707, 'LEMMA_quì'),\n",
       " (-21.474155920751, 'TRI_LEMMA_\"_LEMMA_Hermione_LEMMA_.'),\n",
       " (-21.472182675075995, 'TRI_LEMMA_cos’_LEMMA_essere_LEMMA_l’'),\n",
       " (-21.46414546896072, 'LEMMA_pelle'),\n",
       " (-21.393439715738484, 'LEMMA_orfano'),\n",
       " (-21.294660771208605, 'LEMMA_provocare'),\n",
       " (-21.129712705049926, 'LEMMA_studio'),\n",
       " (-21.09284001384707, 'LEMMA_divertimento'),\n",
       " (-21.044188576263362, 'BI_LEMMA_elemento_LEMMA_:'),\n",
       " (-20.964212940978094, 'LEMMA_piacere'),\n",
       " (-20.916466927841753, 'LEMMA_stasera'),\n",
       " (-20.656224402232795, 'LEMMA_piton'),\n",
       " (-20.622195585946116, 'TRI_LEMMA_,_LEMMA_guardare_LEMMA_occhio'),\n",
       " (-20.527922950582134, 'LEMMA_sangue'),\n",
       " (-20.449544543526688, 'LEMMA_carne'),\n",
       " (-20.39976123414174, 'TRI_LEMMA_il_LEMMA_prossimo_LEMMA_capitolo'),\n",
       " (-20.371060811222872, 'TRI_LEMMA_,_LEMMA_Hermione_LEMMA_?'),\n",
       " (-20.226371922805704, 'BI_LEMMA_ammissione_LEMMA_,'),\n",
       " (-20.19544042708223, 'TRI_LEMMA_Harry_LEMMA_e_LEMMA_Draco'),\n",
       " (-20.032309172823446, 'LEMMA_editore'),\n",
       " (-19.898249839960762, 'LEMMA_firmare'),\n",
       " (-19.842109331936335, 'BI_LEMMA_mantenere_LEMMA_.'),\n",
       " (-19.825182341510605, 'LEMMA_disgustare'),\n",
       " (-19.77134490665519, 'LEMMA_Annie'),\n",
       " (-19.661247913300485, 'LEMMA_eccitare')]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_feats_w_classifier_weight[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
